{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Scan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023, Tri Dao, Albert Gu.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "def selective_scan_fn(\n",
    "    u,\n",
    "    delta,\n",
    "    A,\n",
    "    B,\n",
    "    C,\n",
    "    D=None,\n",
    "    z=None,\n",
    "    delta_bias=None,\n",
    "    delta_softplus=False,\n",
    "    return_last_state=False,\n",
    "):\n",
    "    \"\"\"if return_last_state is True, returns (out, last_state)\n",
    "    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is\n",
    "    not considered in the backward pass.\n",
    "    \"\"\"\n",
    "    return selective_scan_ref(\n",
    "        u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state\n",
    "    )\n",
    "\n",
    "\n",
    "def selective_scan_ref(\n",
    "    u,\n",
    "    delta,\n",
    "    A,\n",
    "    B,\n",
    "    C,\n",
    "    D=None,\n",
    "    z=None,\n",
    "    delta_bias=None,\n",
    "    delta_softplus=False,\n",
    "    return_last_state=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    u: r(B D L)\n",
    "    delta: r(B D L)\n",
    "    A: c(D N) or r(D N)\n",
    "    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
    "    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
    "    D: r(D)\n",
    "    z: r(B D L)\n",
    "    delta_bias: r(D), fp32\n",
    "\n",
    "    out: r(B D L)\n",
    "    last_state (optional): r(B D dstate) or c(B D dstate)\n",
    "    \"\"\"\n",
    "    dtype_in = u.dtype\n",
    "    u = u.float()\n",
    "    delta = delta.float()\n",
    "    if delta_bias is not None:\n",
    "        delta = delta + delta_bias[..., None].float()\n",
    "    if delta_softplus:\n",
    "        delta = F.softplus(delta)\n",
    "    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
    "    is_variable_B = B.dim() >= 3\n",
    "    is_variable_C = C.dim() >= 3\n",
    "    if A.is_complex():\n",
    "        if is_variable_B:\n",
    "            B = torch.view_as_complex(\n",
    "                rearrange(B.float(), \"... (L two) -> ... L two\", two=2)\n",
    "            )\n",
    "        if is_variable_C:\n",
    "            C = torch.view_as_complex(\n",
    "                rearrange(C.float(), \"... (L two) -> ... L two\", two=2)\n",
    "            )\n",
    "    else:\n",
    "        B = B.float()\n",
    "        C = C.float()\n",
    "    x = A.new_zeros((batch, dim, dstate))\n",
    "    ys = []\n",
    "    deltaA = torch.exp(torch.einsum(\"bdl,dn->bdln\", delta, A))\n",
    "    if not is_variable_B:\n",
    "        deltaB_u = torch.einsum(\"bdl,dn,bdl->bdln\", delta, B, u)\n",
    "    else:\n",
    "        if B.dim() == 3:\n",
    "            deltaB_u = torch.einsum(\"bdl,bnl,bdl->bdln\", delta, B, u)\n",
    "        else:\n",
    "            B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
    "            deltaB_u = torch.einsum(\"bdl,bdnl,bdl->bdln\", delta, B, u)\n",
    "    if is_variable_C and C.dim() == 4:\n",
    "        C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
    "    last_state = None\n",
    "    for i in range(u.shape[2]):\n",
    "        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
    "        if not is_variable_C:\n",
    "            y = torch.einsum(\"bdn,dn->bd\", x, C)\n",
    "        else:\n",
    "            if C.dim() == 3:\n",
    "                y = torch.einsum(\"bdn,bn->bd\", x, C[:, :, i])\n",
    "            else:\n",
    "                y = torch.einsum(\"bdn,bdn->bd\", x, C[:, :, :, i])\n",
    "        if i == u.shape[2] - 1:\n",
    "            last_state = x\n",
    "        if y.is_complex():\n",
    "            y = y.real * 2\n",
    "        ys.append(y)\n",
    "    y = torch.stack(ys, dim=2)  # (batch dim L)\n",
    "    out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
    "    if z is not None:\n",
    "        out = out * F.silu(z)\n",
    "    out = out.to(dtype=dtype_in)\n",
    "    return out if not return_last_state else (out, last_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedMambaSE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "# from selective_scan import selective_scan_fn\n",
    "\n",
    "\n",
    "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
    "\n",
    "\n",
    "def flops_selective_scan_ref(\n",
    "    B=1,\n",
    "    L=256,\n",
    "    D=768,\n",
    "    N=16,\n",
    "    with_D=True,\n",
    "    with_Z=False,\n",
    "    with_Group=True,\n",
    "    with_complex=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    u: r(B D L)\n",
    "    delta: r(B D L)\n",
    "    A: r(D N)\n",
    "    B: r(B N L)\n",
    "    C: r(B N L)\n",
    "    D: r(D)\n",
    "    z: r(B D L)\n",
    "    delta_bias: r(D), fp32\n",
    "\n",
    "    ignores:\n",
    "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # fvcore.nn.jit_handles\n",
    "    def get_flops_einsum(input_shapes, equation):\n",
    "        np_arrs = [np.zeros(s) for s in input_shapes]\n",
    "        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n",
    "        for line in optim.split(\"\\n\"):\n",
    "            if \"optimized flop\" in line.lower():\n",
    "                # divided by 2 because we count MAC (multiply-add counted as one flop)\n",
    "                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n",
    "                return flop\n",
    "\n",
    "    assert not with_complex\n",
    "\n",
    "    flops = 0  # below code flops = 0\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        dtype_in = u.dtype\n",
    "        u = u.float()\n",
    "        delta = delta.float()\n",
    "        if delta_bias is not None:\n",
    "            delta = delta + delta_bias[..., None].float()\n",
    "        if delta_softplus:\n",
    "            delta = F.softplus(delta)\n",
    "        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
    "        is_variable_B = B.dim() >= 3\n",
    "        is_variable_C = C.dim() >= 3\n",
    "        if A.is_complex():\n",
    "            if is_variable_B:\n",
    "                B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
    "            if is_variable_C:\n",
    "                C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
    "        else:\n",
    "            B = B.float()\n",
    "            C = C.float()\n",
    "        x = A.new_zeros((batch, dim, dstate))\n",
    "        ys = []\n",
    "        \"\"\"\n",
    "\n",
    "    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n",
    "    if with_Group:\n",
    "        flops += get_flops_einsum(\n",
    "            [[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\"\n",
    "        )\n",
    "    else:\n",
    "        flops += get_flops_einsum(\n",
    "            [[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\"\n",
    "        )\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "        if not is_variable_B:\n",
    "            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
    "        else:\n",
    "            if B.dim() == 3:\n",
    "                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
    "            else:\n",
    "                B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
    "                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
    "        if is_variable_C and C.dim() == 4:\n",
    "            C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
    "        last_state = None\n",
    "        \"\"\"\n",
    "\n",
    "    in_for_flops = B * D * N\n",
    "    if with_Group:\n",
    "        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n",
    "    else:\n",
    "        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n",
    "    flops += L * in_for_flops\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        for i in range(u.shape[2]):\n",
    "            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
    "            if not is_variable_C:\n",
    "                y = torch.einsum('bdn,dn->bd', x, C)\n",
    "            else:\n",
    "                if C.dim() == 3:\n",
    "                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
    "                else:\n",
    "                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
    "            if i == u.shape[2] - 1:\n",
    "                last_state = x\n",
    "            if y.is_complex():\n",
    "                y = y.real * 2\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=2) # (batch dim L)\n",
    "        \"\"\"\n",
    "\n",
    "    if with_D:\n",
    "        flops += B * D * L\n",
    "    if with_Z:\n",
    "        flops += B * D * L\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
    "        if z is not None:\n",
    "            out = out * F.silu(z)\n",
    "        out = out.to(dtype=dtype_in)\n",
    "        \"\"\"\n",
    "\n",
    "    return flops\n",
    "\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    r\"\"\"Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging2D(nn.Module):\n",
    "    r\"\"\"Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            print(\n",
    "                f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True\n",
    "            )\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, : SHAPE_FIX[0], : SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, : SHAPE_FIX[0], : SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, : SHAPE_FIX[0], : SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, : SHAPE_FIX[0], : SHAPE_FIX[1], :]\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H // 2, W // 2, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchExpand2D(nn.Module):\n",
    "    def __init__(self, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim * 2\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(self.dim, dim_scale * self.dim, bias=False)\n",
    "        self.norm = norm_layer(self.dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b h w (p1 p2 c)-> b (h p1) (w p2) c\",\n",
    "            p1=self.dim_scale,\n",
    "            p2=self.dim_scale,\n",
    "            c=C // self.dim_scale,\n",
    "        )\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Final_PatchExpand2D(nn.Module):\n",
    "    def __init__(self, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(self.dim, dim_scale * self.dim, bias=False)\n",
    "        self.norm = norm_layer(self.dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b h w (p1 p2 c)-> b (h p1) (w p2) c\",\n",
    "            p1=self.dim_scale,\n",
    "            p2=self.dim_scale,\n",
    "            c=C // self.dim_scale,\n",
    "        )\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SS2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        # d_state=\"auto\", # 20240109\n",
    "        d_conv=3,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        dropout=0.0,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "        self.in_proj = nn.Linear(\n",
    "            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs\n",
    "        )\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            groups=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            padding=(d_conv - 1) // 2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = (\n",
    "            nn.Linear(\n",
    "                self.d_inner,\n",
    "                (self.dt_rank + self.d_state * 2),\n",
    "                bias=False,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "            nn.Linear(\n",
    "                self.d_inner,\n",
    "                (self.dt_rank + self.d_state * 2),\n",
    "                bias=False,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "            nn.Linear(\n",
    "                self.d_inner,\n",
    "                (self.dt_rank + self.d_state * 2),\n",
    "                bias=False,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "            nn.Linear(\n",
    "                self.d_inner,\n",
    "                (self.dt_rank + self.d_state * 2),\n",
    "                bias=False,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "        )\n",
    "        self.x_proj_weight = nn.Parameter(\n",
    "            torch.stack([t.weight for t in self.x_proj], dim=0)\n",
    "        )  # (K=4, N, inner)\n",
    "        del self.x_proj\n",
    "\n",
    "        self.dt_projs = (\n",
    "            self.dt_init(\n",
    "                self.dt_rank,\n",
    "                self.d_inner,\n",
    "                dt_scale,\n",
    "                dt_init,\n",
    "                dt_min,\n",
    "                dt_max,\n",
    "                dt_init_floor,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "            self.dt_init(\n",
    "                self.dt_rank,\n",
    "                self.d_inner,\n",
    "                dt_scale,\n",
    "                dt_init,\n",
    "                dt_min,\n",
    "                dt_max,\n",
    "                dt_init_floor,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "            self.dt_init(\n",
    "                self.dt_rank,\n",
    "                self.d_inner,\n",
    "                dt_scale,\n",
    "                dt_init,\n",
    "                dt_min,\n",
    "                dt_max,\n",
    "                dt_init_floor,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "            self.dt_init(\n",
    "                self.dt_rank,\n",
    "                self.d_inner,\n",
    "                dt_scale,\n",
    "                dt_init,\n",
    "                dt_min,\n",
    "                dt_max,\n",
    "                dt_init_floor,\n",
    "                **factory_kwargs,\n",
    "            ),\n",
    "        )\n",
    "        self.dt_projs_weight = nn.Parameter(\n",
    "            torch.stack([t.weight for t in self.dt_projs], dim=0)\n",
    "        )  # (K=4, inner, rank)\n",
    "        self.dt_projs_bias = nn.Parameter(\n",
    "            torch.stack([t.bias for t in self.dt_projs], dim=0)\n",
    "        )  # (K=4, inner)\n",
    "        del self.dt_projs\n",
    "\n",
    "        self.A_logs = self.A_log_init(\n",
    "            self.d_state, self.d_inner, copies=4, merge=True\n",
    "        )  # (K=4, D, N)\n",
    "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True)  # (K=4, D, N)\n",
    "\n",
    "        # self.selective_scan = selective_scan_fn\n",
    "        self.forward_core = self.forward_corev0\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
    "        self.out_proj = nn.Linear(\n",
    "            self.d_inner, self.d_model, bias=bias, **factory_kwargs\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else None\n",
    "\n",
    "    @staticmethod\n",
    "    def dt_init(\n",
    "        dt_rank,\n",
    "        d_inner,\n",
    "        dt_scale=1.0,\n",
    "        dt_init=\"random\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init_floor=1e-4,\n",
    "        **factory_kwargs,\n",
    "    ):\n",
    "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(d_inner, **factory_kwargs)\n",
    "            * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        dt_proj.bias._no_reinit = True\n",
    "\n",
    "        return dt_proj\n",
    "\n",
    "    @staticmethod\n",
    "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        if copies > 1:\n",
    "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
    "            if merge:\n",
    "                A_log = A_log.flatten(0, 1)\n",
    "        A_log = nn.Parameter(A_log)\n",
    "        A_log._no_weight_decay = True\n",
    "        return A_log\n",
    "\n",
    "    @staticmethod\n",
    "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
    "        # D \"skip\" parameter\n",
    "        D = torch.ones(d_inner, device=device)\n",
    "        if copies > 1:\n",
    "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
    "            if merge:\n",
    "                D = D.flatten(0, 1)\n",
    "        D = nn.Parameter(D)  # Keep in fp32\n",
    "        D._no_weight_decay = True\n",
    "        return D\n",
    "\n",
    "    def forward_corev0(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack(\n",
    "            [\n",
    "                x.view(B, -1, L),\n",
    "                torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L),\n",
    "            ],\n",
    "            dim=1,\n",
    "        ).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1)  # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\n",
    "            \"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight\n",
    "        )\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(\n",
    "            x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2\n",
    "        )\n",
    "        dts = torch.einsum(\n",
    "            \"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight\n",
    "        )\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L)  # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L)  # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1)  # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1)  # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs,\n",
    "            dts,\n",
    "            As,\n",
    "            Bs,\n",
    "            Cs,\n",
    "            Ds,\n",
    "            z=None,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=False,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = (\n",
    "            torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3)\n",
    "            .contiguous()\n",
    "            .view(B, -1, L)\n",
    "        )\n",
    "        invwh_y = (\n",
    "            torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3)\n",
    "            .contiguous()\n",
    "            .view(B, -1, L)\n",
    "        )\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    # an alternative to forward_corev1\n",
    "    def forward_corev1(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn  # selective_scan_fn_v1\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack(\n",
    "            [\n",
    "                x.view(B, -1, L),\n",
    "                torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L),\n",
    "            ],\n",
    "            dim=1,\n",
    "        ).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1)  # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\n",
    "            \"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight\n",
    "        )\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(\n",
    "            x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2\n",
    "        )\n",
    "        dts = torch.einsum(\n",
    "            \"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight\n",
    "        )\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L)  # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L)  # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L)  # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1)  # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1)  # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs,\n",
    "            dts,\n",
    "            As,\n",
    "            Bs,\n",
    "            Cs,\n",
    "            Ds,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = (\n",
    "            torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3)\n",
    "            .contiguous()\n",
    "            .view(B, -1, L)\n",
    "        )\n",
    "        invwh_y = (\n",
    "            torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3)\n",
    "            .contiguous()\n",
    "            .view(B, -1, L)\n",
    "        )\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x, z = xz.chunk(2, dim=-1)  # (b, h, w, d)\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.act(self.conv2d(x))  # (b, d, h, w)\n",
    "        y1, y2, y3, y4 = self.forward_core(x)\n",
    "        assert y1.dtype == torch.float32\n",
    "        y = y1 + y2 + y3 + y4\n",
    "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
    "        y = self.out_norm(y)\n",
    "        y = y * F.silu(z)\n",
    "        out = self.out_proj(y)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvSSM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 0,\n",
    "        drop_path: float = 0,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        attn_drop_rate: float = 0,\n",
    "        d_state: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(hidden_dim // 2)\n",
    "        self.self_attention = SS2D(\n",
    "            d_model=hidden_dim // 2, dropout=attn_drop_rate, d_state=d_state, **kwargs\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "        self.conv33conv33conv11 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_dim // 2,\n",
    "                out_channels=hidden_dim // 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_dim // 2,\n",
    "                out_channels=hidden_dim // 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_dim // 2,\n",
    "                out_channels=hidden_dim // 2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "            ),\n",
    "        )\n",
    "        self.finalconv11 = nn.Conv2d(\n",
    "            in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, stride=1\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(channel=hidden_dim)\n",
    "        \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        input_left, input_right = input.chunk(2, dim=-1)\n",
    "        x = input_right + self.drop_path(self.self_attention(self.ln_1(input_right)))\n",
    "        input_left = input_left.permute(0, 3, 1, 2).contiguous()\n",
    "        input_left = self.conv33conv33conv11(input_left)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        output = torch.cat((input_left, x), dim=1)\n",
    "        output = self.se(output)\n",
    "        output = self.finalconv11(output).permute(0, 2, 3, 1).contiguous()\n",
    "        return output + input\n",
    "\n",
    "\n",
    "class VSSLayer(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "        d_state=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvSSM(\n",
    "                    hidden_dim=dim,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    attn_drop_rate=attn_drop,\n",
    "                    d_state=d_state,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if True:  # is this really applied? Yes, but been overriden later in VSSM!\n",
    "\n",
    "            def _init_weights(module: nn.Module):\n",
    "                for name, p in module.named_parameters():\n",
    "                    if name in [\"out_proj.weight\"]:\n",
    "                        p = p.clone().detach_()  # fake init, just to keep the seed ....\n",
    "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "\n",
    "            self.apply(_init_weights)\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSLayer_up(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        upsample=None,\n",
    "        use_checkpoint=False,\n",
    "        d_state=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvSSM(\n",
    "                    hidden_dim=dim,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    attn_drop_rate=attn_drop,\n",
    "                    d_state=d_state,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if True:  # is this really applied? Yes, but been overriden later in VSSM!\n",
    "\n",
    "            def _init_weights(module: nn.Module):\n",
    "                for name, p in module.named_parameters():\n",
    "                    if name in [\"out_proj.weight\"]:\n",
    "                        p = p.clone().detach_()  # fake init, just to keep the seed ....\n",
    "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "\n",
    "            self.apply(_init_weights)\n",
    "\n",
    "        if upsample is not None:\n",
    "            self.upsample = upsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.upsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample is not None:\n",
    "            x = self.upsample(x)\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        depths_decoder=[2, 9, 2, 2],\n",
    "        dims=[96, 192, 384, 768],\n",
    "        dims_decoder=[768, 384, 192, 96],\n",
    "        d_state=16,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=True,\n",
    "        use_checkpoint=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2**i_layer) for i_layer in range(self.num_layers)]\n",
    "        self.embed_dim = dims[0]\n",
    "        self.num_features = dims[-1]\n",
    "        self.dims = dims\n",
    "\n",
    "        self.patch_embed = PatchEmbed2D(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=self.embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None,\n",
    "        )\n",
    "\n",
    "        # WASTED absolute position embedding ======================\n",
    "        self.ape = False\n",
    "        # self.ape = False\n",
    "        # drop_rate = 0.0\n",
    "        if self.ape:\n",
    "            self.patches_resolution = self.patch_embed.patches_resolution\n",
    "            self.absolute_pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, *self.patches_resolution, self.embed_dim)\n",
    "            )\n",
    "            trunc_normal_(self.absolute_pos_embed, std=0.02)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "        dpr_decoder = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths_decoder))\n",
    "        ][::-1]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = VSSLayer(\n",
    "                dim=dims[i_layer],\n",
    "                depth=depths[i_layer],\n",
    "                d_state=math.ceil(dims[0] / 6)\n",
    "                if d_state is None\n",
    "                else d_state,  # 20240109\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = (\n",
    "            nn.Linear(self.num_features, num_classes)\n",
    "            if num_classes > 0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "    def _init_weights(self, m: nn.Module):\n",
    "        \"\"\"\n",
    "        out_proj.weight which is previously initilized in ConvSSM, would be cleared in nn.Linear\n",
    "        no fc.weight found in the any of the model parameters\n",
    "        no nn.Embedding found in the any of the model parameters\n",
    "        so the thing is, ConvSSM initialization is useless\n",
    "\n",
    "        Conv2D is not intialized !!!\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"absolute_pos_embed\"}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"relative_position_bias_table\"}\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_backbone(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the configuration dictionary\n",
    "cfg = {\n",
    "    'model_name': 'MedMamba',\n",
    "    'ckpt_path': 'models',\n",
    "    'history_path': 'evaluation',\n",
    "    'train_data_path': 'train',\n",
    "    'val_data_path': 'val',\n",
    "    'test_data_path': 'test',\n",
    "    'image_size': 128,\n",
    "    'classes': 6,\n",
    "    'train': {\n",
    "       \n",
    "        'batch_size': 16,  \n",
    "        'num_epochs': 30, \n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "class_indices = {\n",
    "    \"0\": \"Other\",\n",
    "    \"1\": \"Maternal cervix\",\n",
    "    \"2\": \"Fetal abdomen\",\n",
    "    \"3\": \"Fetal brain\",\n",
    "    \"4\": \"Fetal femur\",\n",
    "    \"5\": \"Fetal thorax\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = cfg['train']['batch_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main(cfg):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.RandomResizedCrop(cfg['image_size']),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=cfg['train_data_path'], transform=data_transform[\"train\"]\n",
    "    )\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    cla_dict = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "    json_str = json.dumps(cla_dict, indent=4)\n",
    "    with open(f\"{cfg['history_path']}class_indices.json\", \"w\") as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    batch_size = cfg['train']['batch_size']\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print(\"Using {} dataloader workers every process\".format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw\n",
    "    )\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(\n",
    "        root=cfg['val_data_path'], transform=data_transform[\"val\"]\n",
    "    )\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(\n",
    "        validate_dataset, batch_size=batch_size, shuffle=False, num_workers=nw\n",
    "    )\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))\n",
    "\n",
    "    net = VSSM(num_classes=cfg['classes'])\n",
    "    net.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    epochs = cfg['train']['num_epochs']\n",
    "    best_acc = 0.0\n",
    "    save_path = f\"{cfg['ckpt_path']}{cfg['model_name']}Net.pth\"\n",
    "    early_stopping_patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images.to(device))\n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(\n",
    "                epoch + 1, epochs, loss\n",
    "            )\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Training accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        net.eval()\n",
    "        acc = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(validate_loader, file=sys.stdout):\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print(\n",
    "            \"[epoch %d] train_loss: %.3f, train_accuracy: %.3f, val_accuracy: %.3f\"\n",
    "            % (epoch + 1, running_loss / len(train_loader), train_accuracy, val_accurate)\n",
    "        )\n",
    "\n",
    "        with open(f\"{cfg['history_path']}train_metrics.txt\", \"a\") as f:\n",
    "            f.write(\n",
    "                f\"epoch: {epoch + 1}, train_loss: {running_loss / len(train_loader)}, train_accuracy: {train_accuracy}, val_accuracy: {val_accurate}\\n\"\n",
    "            )\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "            print(\"model saved\")\n",
    "            patience_counter = 0  # reset patience for a new best\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
